# -*- coding: utf-8 -*-
"""projet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Lh5Rv-dk4HrXAw1nb1P2rOepgkz52VdR
"""

# ===== PASSO 1: Configuração inicial =====
import zipfile
import os
import cv2
import numpy as np
from skimage.feature import greycomatrix, greycoprops
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
import matplotlib.pyplot as plt

print("Bibliotecas carregadas!")

!pip install scikit-image==0.21.0

from skimage.feature import graycomatrix, graycoprops

!pip install numpy==1.26.4
!pip install scikit-image==0.21.0

import numpy as np
from skimage.feature import graycomatrix, graycoprops

!pip install numpy==1.26.4 scikit-image==0.21.0

import zipfile
import os
import cv2
import numpy as np
from skimage.feature import graycomatrix, graycoprops
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
import matplotlib.pyplot as plt

print("Bibliotecas carregadas!")

zip_path = "/content/coffee.zip"  # coloque o nome do seu arquivo
extract_path = "/content/dataset"

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

print("Dataset extraído!")
os.listdir(extract_path)

import os
os.listdir("/content")

from google.colab import files
uploaded = files.upload()

import os
os.listdir("/content")

zip_path = "/content/archive.zip"
extract_path = "/content/dataset"

import zipfile

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

print("OK! Dataset extraído.")
os.listdir(extract_path)

os.listdir(extract_path)

import os
import cv2
import numpy as np

data = []
labels = []

base_path = "/content/dataset"

folders = ["train", "test"]  # vamos ler as duas pastas

for folder in folders:
    folder_path = os.path.join(base_path, folder)
    print("Processando:", folder_path)

    for class_name in os.listdir(folder_path):
        class_folder = os.path.join(folder_path, class_name)

        if not os.path.isdir(class_folder):
            continue

        print("  Classe:", class_name)

        for img_name in os.listdir(class_folder):
            img_path = os.path.join(class_folder, img_name)

            img = cv2.imread(img_path)
            if img is None:
                continue

            img = cv2.resize(img, (224, 224))

            features = extract_haralick_features(img)

            data.append(features)
            labels.append(class_name)

data = np.array(data)
labels = np.array(labels)

print("\nExtração concluída!")
print("Shape dos dados:", data.shape)
print("Classes únicas:", np.unique(labels))

# ===== Célula única: definir função, extrair features, treinar e avaliar =====
import os
import cv2
import numpy as np
from skimage.feature import graycomatrix, graycoprops
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
import matplotlib.pyplot as plt

# ---- Função Haralick ----
def extract_haralick_features(image):
    """
    Recebe BGR image (OpenCV), retorna vetor com 6 features Haralick (médias sobre ângulos).
    """
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

    # Se a imagem tem valores que não são inteiros pequenos, converta para uint8
    if gray.dtype != np.uint8:
        gray = cv2.normalize(gray, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)

    # GLCM
    glcm = graycomatrix(
        gray,
        distances=[1],
        angles=[0, np.pi/4, np.pi/2, 3*np.pi/4],
        symmetric=True,
        normed=True
    )

    features = [
        graycoprops(glcm, 'contrast').mean(),
        graycoprops(glcm, 'dissimilarity').mean(),
        graycoprops(glcm, 'homogeneity').mean(),
        graycoprops(glcm, 'ASM').mean(),
        graycoprops(glcm, 'energy').mean(),
        graycoprops(glcm, 'correlation').mean()
    ]
    return features

# ---- Ler imagens (train + test) e extrair features ----
base_path = "/content/dataset"
folders = ["train", "test"]

data = []
labels = []
skipped = 0

for folder in folders:
    folder_path = os.path.join(base_path, folder)
    if not os.path.exists(folder_path):
        print("Pasta não encontrada:", folder_path)
        continue

    print("Processando:", folder_path)
    for class_name in sorted(os.listdir(folder_path)):
        class_folder = os.path.join(folder_path, class_name)
        if not os.path.isdir(class_folder):
            continue

        print("  Classe:", class_name)
        for img_name in os.listdir(class_folder):
            img_path = os.path.join(class_folder, img_name)
            try:
                img = cv2.imread(img_path)
                if img is None:
                    skipped += 1
                    continue

                img = cv2.resize(img, (224, 224))
                features = extract_haralick_features(img)
                data.append(features)
                labels.append(class_name)
            except Exception as e:
                skipped += 1
                # opcional: print("Erro em", img_path, e)

data = np.array(data)
labels = np.array(labels)

print("\nExtração concluída!")
print("Total imagens processadas:", len(labels))
print("Arquivos pulados:", skipped)
print("Shape dos dados:", data.shape)
print("Classes encontradas:", np.unique(labels))

# ---- Verificar que há pelo menos 2 classes e dados suficientes ----
if len(np.unique(labels)) < 2 or len(labels) < 10:
    raise RuntimeError("Poucas classes ou poucas imagens. Verifique o dataset.")

# ---- Separar treino/teste (random split para avaliação interna) ----
X_train, X_test, y_train, y_test = train_test_split(
    data, labels, test_size=0.2, random_state=42, stratify=labels
)

print("\nSplit feito. Treino:", X_train.shape, "Teste:", X_test.shape)

# ---- Treinar SVM ----
clf = SVC(kernel='rbf', probability=True)
clf.fit(X_train, y_train)
print("Modelo treinado!")

# ---- Avaliação ----
y_pred = clf.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print("\nAcurácia:", acc)
print("\nRelatório de Classificação:")
print(classification_report(y_test, y_pred))

cm = confusion_matrix(y_test, y_pred)
print("Matriz de Confusão:\n", cm)

plt.figure(figsize=(5,4))
plt.imshow(cm, cmap='Blues')
plt.title("Matriz de Confusão")
plt.colorbar()
plt.xlabel("Predito")
plt.ylabel("Real")
plt.tight_layout()
plt.show()



"""-----melhoria

"""

import os
import cv2
import numpy as np
import zipfile
from skimage.feature import graycomatrix, graycoprops
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

zip_path = "/content/archive.zip"
extract_path = "/content/dataset"

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

print(os.listdir(extract_path))

def extract_haralick_features(img):
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    gray = cv2.normalize(gray, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)

    glcm = graycomatrix(gray, distances=[1], angles=[0], levels=256, symmetric=True, normed=True)

    feats = [
        graycoprops(glcm, 'contrast')[0, 0],
        graycoprops(glcm, 'dissimilarity')[0, 0],
        graycoprops(glcm, 'homogeneity')[0, 0],
        graycoprops(glcm, 'energy')[0, 0],
        graycoprops(glcm, 'correlation')[0, 0],
        graycoprops(glcm, 'ASM')[0, 0]
    ]
    return np.array(feats, dtype=float)

def extract_color_hist(img, bins=32):
    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
    hist = cv2.calcHist([hsv], [0,1,2], None,
                        [bins, bins, bins], [0,180,0,256,0,256])
    hist = cv2.normalize(hist, hist).flatten()
    return hist

dataset_dir = "/content/dataset/train"   # pasta extraída
data = []
labels = []

classes = sorted(os.listdir(dataset_dir))
print("Classes detectadas:", classes)

skip = 0
total = 0

for cls in classes:
    cls_path = os.path.join(dataset_dir, cls)
    if not os.path.isdir(cls_path):
        continue

    print("Processando classe:", cls)

    for img_name in os.listdir(cls_path):
        img_path = os.path.join(cls_path, img_name)

        img = cv2.imread(img_path)
        if img is None:
            skip += 1
            continue

        img = cv2.resize(img, (224, 224))

        hara = extract_haralick_features(img)
        hist = extract_color_hist(img)

        feats = np.hstack([hara, hist])
        data.append(feats)
        labels.append(cls)

        total += 1

print("\nTotal processado:", total)
print("Pulados:", skip)

data = np.array(data)
labels = np.array(labels)

print("Shape final dos dados:", data.shape)

X_train, X_test, y_train, y_test = train_test_split(
    data, labels, test_size=0.20, random_state=42, stratify=labels
)

pipeline = Pipeline([
    ("scaler", StandardScaler()),
    ("svm", SVC())
])

params = {
    "svm__kernel": ["rbf"],
    "svm__C": [1, 10, 100],
    "svm__gamma": ["scale", 0.01, 0.001]
}

grid = GridSearchCV(pipeline, params, cv=3, verbose=1, n_jobs=-1)
grid.fit(X_train, y_train)

print("Melhores parâmetros:", grid.best_params_)

y_pred = grid.predict(X_test)

print("\nAcurácia:", grid.score(X_test, y_test))
print("\nRelatório:")
print(classification_report(y_test, y_pred))

cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(7,6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=classes, yticklabels=classes)
plt.xlabel("Predito")
plt.ylabel("Real")
plt.title("Matriz de Confusão")
plt.show()